{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import seaborn as sns\n",
    "import regex as re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('/Users/aaryansingh/Non-Native Influences Investigation/Input/cmudict-0.7b.txt') as f:\n",
    "    lines = f.readlines()\n",
    "    #remove all numbers\n",
    "    lines = [re.sub(r'\\d', '', line) for line in lines]\n",
    "    #split by 2 spaces\n",
    "    lines = [re.split(r'  ', line) for line in lines]\n",
    "    with open('phonemes_cmu.txt', 'w') as f:\n",
    "        for line in lines:\n",
    "            f.write(line[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['AH'], ['EY'], ['EY', 'Z'], ['EY'], ['EY', 'Z']]\n"
     ]
    }
   ],
   "source": [
    "#  create array of words for each sentence\n",
    "sentences = []\n",
    "with open ('phonemes_cmu.txt') as f:\n",
    "    lines = f.readlines()\n",
    "    #split the words in each line by 1st space\n",
    "    for line in lines:\n",
    "        line = line.replace('  ', ' ')\n",
    "        line = line.replace('\\n', '')\n",
    "        words = line.split(' ')\n",
    "        sentences.append(words)\n",
    "print(sentences[0:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "# Parameters for the Word2Vec model\n",
    "vector_size = 20\n",
    "window = 1\n",
    "min_count = 3\n",
    "initial_epochs = 100  # Number of epochs for the initial training phase\n",
    "\n",
    "# Initialize and build the model\n",
    "model = Word2Vec(vector_size=vector_size, window=window, min_count=min_count, workers=4)\n",
    "model.build_vocab(sentences)\n",
    "\n",
    "# Start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Define parameters for monitoring\n",
    "max_time = 3 * 60  # 5 minutes in seconds\n",
    "epoch_increment = 100  # Number of epochs to train in each iteration\n",
    "\n",
    "# Training loop\n",
    "while True:\n",
    "    model.train(sentences, total_examples=model.corpus_count, epochs=epoch_increment)\n",
    "    \n",
    "    # Check elapsed time\n",
    "    elapsed_time = time.time() - start_time\n",
    "    if elapsed_time >= max_time:\n",
    "        break\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model.save('cmu_word2vec_.model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
